{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, jieba, os, random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayesClassifier():\n",
    "    \"\"\"\n",
    "    读取数据集，按10折交叉验证将数据集随机均分为10份，分别处理好评集与差评集。\n",
    "    \"\"\"\n",
    "    def __init__(self, pos_dir, neg_dir):\n",
    "        global cn_stopwords\n",
    "        cn_stopwords = []\n",
    "        with open('sw_for_comments/cn_stopWords.txt') as f:\n",
    "            for line in f:\n",
    "                cn_stopwords.append(line.strip())\n",
    "        \n",
    "        print(\"正在准备训练和测试数据，请稍后...\")\n",
    "        self.pos_split = self.process_file_list(pos_dir)\n",
    "        self.neg_split = self.process_file_list(neg_dir)\n",
    "        self.jieba_add()\n",
    "\n",
    "        \n",
    "    def jieba_add(self):\n",
    "        \"\"\"\n",
    "        增加分词库的词语搭配\n",
    "        \"\"\"\n",
    "        jieba.add_word('不好看', freq=None, tag=None)\n",
    "        jieba.add_word('不喜欢', freq=None, tag=None)\n",
    "        jieba.add_word('不觉得好看', freq=None, tag=None)\n",
    "        jieba.add_word('哪里好看', freq=None, tag=None)\n",
    "        \n",
    "\n",
    "    def get_file_list(self, basedir):\n",
    "        \"\"\"\n",
    "        遍历 basedir 目录下所有 txt 文件，组成一个列表\n",
    "        \"\"\"\n",
    "        file_list = []\n",
    "        for parent, dirnames, filenames in os.walk(basedir):\n",
    "            for filename in filenames:\n",
    "                # 取扩展名\n",
    "                extension = filename.split('.')[-1]\n",
    "                if extension == 'txt':\n",
    "                    file_list.append(os.path.join(parent, filename))\n",
    "        return file_list\n",
    "\n",
    "\n",
    "    def process_file_list(self, basedir):\n",
    "        \"\"\"\n",
    "        处理所有 txt 文件\n",
    "        得到该目录下所包含的评论总数\n",
    "        将评论集均分为 10 份\n",
    "        \"\"\"\n",
    "        file_list = self.get_file_list(basedir)\n",
    "        \n",
    "        all_comments = []\n",
    "        for file in file_list:\n",
    "            all_comments += self.get_comment_list(file)\n",
    "        \n",
    "        random.shuffle(all_comments) # 将数组中的元素随机排序\n",
    "        \n",
    "        comments_split = []\n",
    "        split_len = len(all_comments) // 10\n",
    "\n",
    "        print(\"---正在将数据集随机排序并均等分割为 10 份---\")\n",
    "        for i in range(10):\n",
    "            start = i * split_len\n",
    "            end = (i + 1) * split_len\n",
    "            comments_split.append(all_comments[start:end])\n",
    "\n",
    "        return comments_split\n",
    "    \n",
    "    \n",
    "    def get_comment_list(self, txt):\n",
    "        \"\"\"\n",
    "        输入：txt格式的评论文件，一行一条评论；\n",
    "        输出：评论列表，列表的元素个数就是该文件包含的评论数量。\n",
    "        \"\"\"\n",
    "        comment_list = []\n",
    "        \n",
    "        with open(txt) as f:\n",
    "            for line in f:\n",
    "                if line.strip(): # 检测该行是否为空\n",
    "                    comment_list.append(line.strip())\n",
    "        return comment_list\n",
    "\n",
    "    \n",
    "    def build_dataset(self, comment_list):\n",
    "        \"\"\"\n",
    "        传入两个参数：评论列表\n",
    "        提取每条影评的特征词，\n",
    "        并计算每个词的出现次数，存在 dict 结构中\n",
    "        \"\"\"\n",
    "        feature_words_dict = dict()\n",
    "        \n",
    "        for comment in comment_list:\n",
    "            feature_words = self.find_feature_words(comment)\n",
    "            for word in feature_words:\n",
    "                try:\n",
    "                    feature_words_dict[word] +=1\n",
    "                except KeyError:\n",
    "                    \"\"\"\n",
    "                    若该词还未出现在词典中，则新建条目，\n",
    "                    并初始化频数为 1\n",
    "                    \"\"\"\n",
    "                    feature_words_dict[word] = 1\n",
    "                    \n",
    "        return feature_words_dict\n",
    "    \n",
    "    \n",
    "    def find_feature_words(self, comment):\n",
    "        \"\"\"\n",
    "        输入：单条评论\n",
    "        处理：取中文，分词，去停用词\n",
    "        输出：特征词列表\n",
    "        \"\"\"\n",
    "        pattern = re.compile(u'[\\u4e00-\\u9fa5]+')\n",
    "        comment = re.findall(pattern, comment) #匹配中文字符\n",
    "        comment = ''.join(comment) #将列表连接成一个字符串\n",
    "        \n",
    "        temp = jieba.cut(comment, cut_all=False)\n",
    "        w_list = ' '.join(temp).split()\n",
    "        feature_list = [w for w in w_list if w not in cn_stopwords]\n",
    "        return feature_list\n",
    "    \n",
    "\n",
    "    def train_validate(self, pos_split, neg_split):\n",
    "        \"\"\"\n",
    "        输入：划分好的数据集（10份）\n",
    "        处理：按朴素贝叶斯算法进行训练，使用十折交叉验证处理测试数据，计算准确率和召回率\n",
    "        输出：模型的性能评估\n",
    "        \"\"\"\n",
    "        index = 0\n",
    "        # 初始化 P，R 数组\n",
    "        P = [0 for i in range(10)]\n",
    "        R = [0 for i in range(10)]\n",
    "        \n",
    "        while(True):\n",
    "            print(\"******正在进行第 %s 次验证******\" % str(index + 1))\n",
    "            \"\"\"\n",
    "            ---变量说明---\n",
    "            test_dataset: 测试集\n",
    "            pos_split[index]: 测试集中的好评集\n",
    "            neg_split[index]: 测试集中的差评集\n",
    "            \"\"\"\n",
    "            #test_dataset = pos_split[index] + neg_split[index]\n",
    "            \n",
    "            pos_train = []\n",
    "            neg_train = []\n",
    "            for i in range(10):\n",
    "                if i != index:\n",
    "                    pos_train += pos_split[i]\n",
    "                    neg_train += neg_split[i]\n",
    "                    \n",
    "            print(\"正在计算词频字典...\")\n",
    "            pos_feature_words_dict = self.build_dataset(pos_train)\n",
    "            neg_feature_words_dict = self.build_dataset(neg_train)\n",
    "            print(\"正在构建词语的好评权重...\")\n",
    "            pos_weights = self.compute_words_weights(pos_feature_words_dict, neg_feature_words_dict)\n",
    "            \n",
    "            print(\"正在对测试集进行验证\")\n",
    "            TP = 0 # 真正例\n",
    "            FP = 0 # 假正例\n",
    "            FN = 0 # 假反例\n",
    "            \n",
    "            for review in pos_split[index]:\n",
    "                result = self.classify(review, pos_weights)\n",
    "                if result == 1:\n",
    "                    TP += 1\n",
    "                elif result == 0:\n",
    "                    FN += 1\n",
    "                    \n",
    "            for review in neg_split[index]:\n",
    "                result = self.classify(review, pos_weights)\n",
    "                if result == 1:\n",
    "                    FP += 1\n",
    "            \n",
    "            print(\"正在计算准确率和召回率...\")\n",
    "            P[index] = round((TP / (TP + FP)) * 100, 6)\n",
    "            R[index] = round((TP / (TP + FN)) * 100, 6)\n",
    "            print(\"准确率为：%s\" % str(P[index]))\n",
    "            print(\"召回率为：%s\" % str(R[index]))\n",
    "            \n",
    "            index += 1\n",
    "            if index > 9:\n",
    "                break\n",
    "                \n",
    "        print(\"10折交叉验证后的准确率均值为：%s\" % str(sum(P) / 10))\n",
    "        print(\"10折交叉验证后的召回率均值为：%s\" % str(sum(R) / 10))\n",
    "            \n",
    "    \n",
    "            \n",
    "    def classify(self, review, words_weight):\n",
    "        \"\"\"\n",
    "        －－－影评分类器－－－\n",
    "\n",
    "        先对单条影评进行分词，去除停用词，提取特征词。\n",
    "        从训练好的权重表中取每个词的好评权重，将权重相加，\n",
    "        若和大于 0，则属于好评，否则属于差评。\n",
    "\n",
    "        \"\"\"\n",
    "        feature_list = self.find_feature_words(review)\n",
    "\n",
    "        # 计算该条影评的好评权重值\n",
    "        pos_weight_sum = 0\n",
    "        for feature in feature_list:\n",
    "            \"\"\"\n",
    "            在权重表中搜索特征词的权重，若找不到条目，则视为在训练集中未出现过的特征；\n",
    "            \"\"\"\n",
    "            if words_weight.get(feature):\n",
    "                pos_weight_sum += words_weight.get(feature)\n",
    "            else:\n",
    "                pos_weight_sum += words_weight.get('new')\n",
    "\n",
    "        pos_weight_sum += words_weight.get('pos_cate')\n",
    "\n",
    "        # 1 代表好评，0 代表差评\n",
    "        if pos_weight_sum > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    def compute_words_weights(self, pos_dict, neg_dict):\n",
    "        \"\"\"\n",
    "        1. 输入：好评特征集、差评特征集；\n",
    "        2. 处理：先分别对好评集和差评集计算词频，\n",
    "            再计算每个词语的权重，组成一张权重表，考虑词语不存在的情况；\n",
    "\n",
    "        3. 默认计算词语属于好评的权重，判断的时候权重和 > 0，则属于好评；\n",
    "        4. 对重复词语采用多项式模型，并作平滑处理。\n",
    "        \"\"\"\n",
    "#         # 计算特征集中不重复的词语个数，为平滑处理做准备\n",
    "#         pos_feature_words_sum = len(pos_dict)\n",
    "#         neg_feature_words_sum = len(neg_dict)\n",
    "        \n",
    "        # 计算训练集中的词汇量大小，为平滑处理做准备\n",
    "        # 首先筛选出只出现在好评集中的词汇，再与差评的词汇表相加，得到训练集的词汇表\n",
    "        pos_vocabulary = [w for w in pos_dict.keys() if w not in neg_dict.keys()]\n",
    "        vocab_size = len(pos_vocabulary) + len(neg_dict)\n",
    "\n",
    "        # 统计全部好评特征词出现次数的总和\n",
    "        pos_values_all = sum(pos_dict.values())\n",
    "        # 统计全部差评特征词出现次数的总和\n",
    "        neg_values_all = sum(neg_dict.values())\n",
    "\n",
    "        # 处理零概率事件\n",
    "        not_exist_pos = 1 / (pos_values_all + vocab_size)\n",
    "        not_exist_neg = 1 / (neg_values_all + vocab_size)\n",
    "\n",
    "        # 处理好评特征词典，输出词频\n",
    "        pos_prob_dic = dict()\n",
    "        for key, value in pos_dict.items():\n",
    "            prob = (value + 1) / (pos_values_all + vocab_size)\n",
    "            pos_prob_dic[key] = prob\n",
    "\n",
    "        # 处理差评特征词典，输出词频\n",
    "        neg_prob_dic = dict()\n",
    "        for key, value in neg_dict.items():\n",
    "            prob = (value + 1) / (neg_values_all + vocab_size)\n",
    "            neg_prob_dic[key] = prob\n",
    "\n",
    "\n",
    "        # 构建词语权重表\n",
    "        weight_table = dict()\n",
    "        for word, pos_prob in pos_prob_dic.items():\n",
    "            if neg_prob_dic.get(word):\n",
    "                neg_prob = neg_prob_dic.get(word)\n",
    "            else:\n",
    "                neg_prob = not_exist_neg\n",
    "            weight_table[word] = math.log(pos_prob / neg_prob)\n",
    "\n",
    "\n",
    "        for word, neg_prob in neg_prob_dic.items():\n",
    "            if not weight_table.get(word): # 若权重表中该词语已存在，\n",
    "                if pos_prob_dic.get(word):\n",
    "                    pos_prob = pos_prob_dic.get(word)\n",
    "                else:\n",
    "                    pos_prob = not_exist_pos\n",
    "                weight_table[word] = math.log(pos_prob / neg_prob)\n",
    "\n",
    "        weight_table['new'] = math.log(not_exist_pos / not_exist_neg)\n",
    "\n",
    "        weight_table['pos_cate'] = math.log(95610 / 75432) # 计算好评集的权重\n",
    "        \n",
    "        return weight_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在准备训练和测试数据，请稍后...\n",
      "---正在将数据集随机排序并均等分割为 10 份---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/ym/q2pw4gf16j33yn01zdhqbhnr0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---正在将数据集随机排序并均等分割为 10 份---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.242 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "test = NaiveBayesClassifier('dataset/pos/', 'dataset/neg/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******正在进行第 1 次验证******\n",
      "正在计算词频字典...\n",
      "正在构建词语的好评权重...\n",
      "正在对测试集进行验证\n",
      "正在计算准确率和召回率...\n",
      "准确率为：82.574003\n",
      "召回率为：84.09647\n",
      "******正在进行第 2 次验证******\n",
      "正在计算词频字典...\n",
      "正在构建词语的好评权重...\n",
      "正在对测试集进行验证\n",
      "正在计算准确率和召回率...\n",
      "准确率为：82.446809\n",
      "召回率为：83.974135\n",
      "******正在进行第 3 次验证******\n",
      "正在计算词频字典...\n",
      "正在构建词语的好评权重...\n",
      "正在对测试集进行验证\n",
      "正在计算准确率和召回率...\n",
      "准确率为：82.397782\n",
      "召回率为：83.117791\n",
      "******正在进行第 4 次验证******\n",
      "正在计算词频字典...\n",
      "正在构建词语的好评权重...\n",
      "正在对测试集进行验证\n",
      "正在计算准确率和召回率...\n",
      "准确率为：82.742521\n",
      "召回率为：83.624607\n",
      "******正在进行第 5 次验证******\n",
      "正在计算词频字典...\n",
      "正在构建词语的好评权重...\n",
      "正在对测试集进行验证\n",
      "正在计算准确率和召回率...\n",
      "准确率为：83.125864\n",
      "召回率为：84.026564\n",
      "******正在进行第 6 次验证******\n",
      "正在计算词频字典...\n",
      "正在构建词语的好评权重...\n",
      "正在对测试集进行验证\n",
      "正在计算准确率和召回率...\n",
      "准确率为：82.673438\n",
      "召回率为：83.930444\n",
      "******正在进行第 7 次验证******\n",
      "正在计算词频字典...\n",
      "正在构建词语的好评权重...\n",
      "正在对测试集进行验证\n",
      "正在计算准确率和召回率...\n",
      "准确率为：82.480687\n",
      "召回率为：83.965397\n",
      "******正在进行第 8 次验证******\n",
      "正在计算词频字典...\n",
      "正在构建词语的好评权重...\n",
      "正在对测试集进行验证\n",
      "正在计算准确率和召回率...\n",
      "准确率为：83.089445\n",
      "召回率为：83.8518\n",
      "******正在进行第 9 次验证******\n",
      "正在计算词频字典...\n",
      "正在构建词语的好评权重...\n",
      "正在对测试集进行验证\n",
      "正在计算准确率和召回率...\n",
      "准确率为：82.616534\n",
      "召回率为：84.09647\n",
      "******正在进行第 10 次验证******\n",
      "正在计算词频字典...\n",
      "正在构建词语的好评权重...\n",
      "正在对测试集进行验证\n",
      "正在计算准确率和召回率...\n",
      "准确率为：82.374101\n",
      "召回率为：84.044041\n",
      "10折交叉验证后的准确率均值为：82.65211839999999\n",
      "10折交叉验证后的召回率均值为：83.8727719\n"
     ]
    }
   ],
   "source": [
    "# 拉普拉斯修正\n",
    "test.train_validate(test.pos_split, test.neg_split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
